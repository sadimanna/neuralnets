{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inception-v2 on EMNIST Digits.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "lUoxCKp5lZ5K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.saved_model import tag_constants\n",
        "import numpy as np\n",
        "import time, random\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import warnings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AK0lICAQplMj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "warnings.simplefilter(\"ignore\", FutureWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6If9W0t2DIkf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#DATASET UPLOADS\n",
        "upload = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eHb1smZVEC1E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#NECESSARY TO ACCESS UPLOADED FILES, IDK WHY\n",
        "for k,v in uploaded.keys():\n",
        "  open(k,'wb').write(v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ikep1j0MGF07",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_images = np.load('emnist_digits_training_images.npy')\n",
        "training_labels = np.load('emnist_digits_training_labels.npy')\n",
        "test_images = np.load('emnist_digits_test_images.npy')\n",
        "test_labels = np.load('emnist_digits_test_labels.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oc7UBefEqqEC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_one_hot(labels,w,ls):\n",
        "  return np.eye(10)[labels.astype(int)] #.astype(int)\n",
        "\n",
        "def loss2(logits,onehotlabs,w,ls):\n",
        "  return tf.losses.softmax_cross_entropy(onehotlabs,logits,weights=w,label_smoothing=ls,reduction=None)\n",
        "\n",
        "def aux_loss(logits,labels):\n",
        "  return tf.reduce_mean(loss2(logits,get_one_hot(labels),0.4,0.1),name='aux_cross_entropy')\n",
        "\n",
        "def loss(aux_logits,logits,labels):\n",
        "\tlabels = tf.reshape(tf.cast(labels,tf.int64),[-1])\n",
        "\t#cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits,name='cross_entropy_per_example')\n",
        "\tcross_entropy_mean = tf.reduce_mean(loss2(logits,get_one_hot(labels),1.0,0.1),name='final_cross_entropy')\n",
        "  \n",
        "  total_ce_mean = tf.add(aux_loss(aux_logits,labels),cross_entropy_mean,name='aux_plus_final')\n",
        "\t\n",
        "  total_loss = tf.add(tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)),total_ce_mean,name='total_loss')\n",
        "\t\n",
        "  return total_loss\n",
        "\n",
        "def accuracy(logits,true_labels):\n",
        "\tpred_labels = tf.argmax(logits,1)\n",
        "\ttrue_labels = tf.cast(true_labels,tf.int64)\n",
        "\t#print pred_labels.get_shape().as_list(),true_labels\n",
        "\tcorrect_pred = tf.cast(tf.equal(pred_labels, true_labels), tf.float32)\n",
        "\taccuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
        "\treturn accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ex_V9IWUAGSJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Abz08cIHquWW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.float32,shape=[None,28,28],name='x')\n",
        "y = tf.placeholder(tf.int64,shape=[None],name='y')\n",
        "keep_prob = tf.placeholder(tf.float32,shape=(),name='keep_prob')\n",
        "#lr = tf.placeholder(tf.float32,shape=())\n",
        "mode = tf.placeholder(tf.bool,shape=(),name='mode')\n",
        "\n",
        "aug_img = tf.placeholder(tf.float32,shape=[28,28],name='aug_img')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SXVZch4aq_Ln",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_rotate = tf.contrib.image.rotate(aug_img,-0.5+np.random.random(),interpolation='BILINEAR')\n",
        "img_affine = tf.contrib.image.transform(aug_img,[np.random.random() for i in xrange(8)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jfRi6cNrzAns",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#INCEPTION MODULE A\n",
        "def inceptionA(name,base_inp):\n",
        "  with tf.variable_scope(name):\n",
        "    br1 = tf.layers.conv2d(base_inp,filters=64,kernel_size=1,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b1_1x1')\n",
        "    br1 = tf.layers.conv2d(br1,filters=96,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b1_3x3_1')\n",
        "    br1 = tf.layers.conv2d(br1,filters=96,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b1_3x3_2')\n",
        "    \n",
        "    br2 = tf.layers.conv2d(base_inp,filters=48,kernel_size=1,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b2_1x1')\n",
        "    br2 = tf.layers.conv2d(br2,filters=64,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b2_3x3')\n",
        "    \n",
        "    br3 = tf.layers.average_pooling2d(base_inp,pool_size=3,strides=1,padding='same',name='incA_b3_avg_pool')\n",
        "    br3 = tf.layers.conv2d(br3,filters=64,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b3_1x1')\n",
        "    \n",
        "    br4 = tf.layers.conv2d(base_inp,filters=64,kernel_size=1,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b4_1x1')\n",
        "    \n",
        "    br_concat = tf.concat([br1,br2,br3,br4],3, name+'concat')\n",
        "    \n",
        "    return branch_concat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lRIHM2DczAq7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#INCEPTION MODULE B\n",
        "def inceptionB(name,base_inp):\n",
        "  with tf.variable_scope(name):\n",
        "    br1 = tf.layers.conv2d(base_inp,filters=160,kernel_size=1,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b1_1x1')\n",
        "    br1 = tf.layers.conv2d(br1,filters=160,kernel_size=[3,1],strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b1_3x1_1')\n",
        "    br1 = tf.layers.conv2d(br1,filters=160,kernel_size=[1,3],strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b1_1x3_1')\n",
        "    br1 = tf.layers.conv2d(br1,filters=160,kernel_size=[3,1],strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b1_3x1_2')\n",
        "    br1 = tf.layers.conv2d(br1,filters=192,kernel_size=[1,3],strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b1_1x3_2')\n",
        "    \n",
        "    br2 = tf.layers.conv2d(base_inp,filters=160,kernel_size=1,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b2_1x1')\n",
        "    br2 = tf.layers.conv2d(br2,filters=160,kernel_size=[3,1],strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b2_3x1')\n",
        "    br2 = tf.layers.conv2d(br2,filters=192,kernel_size=[1,3],strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b2_1x3')\n",
        "    \n",
        "    br3 = tf.layers.average_pooling2d(base_inp,pool_size=3,strides=1,padding='same',name='incB_b3_avg_pool')\n",
        "    br3 = tf.layers.conv2d(br3,filters=192,kernel_size=1,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b3_1x1')\n",
        "    \n",
        "    br4 = tf.layers.conv2d(base_inp,filters=192,kernel_size=1,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b4_1x1')\n",
        "    \n",
        "    br_concat = tf.concat([br1,br2,br3,br4],3,name=name+'concat')\n",
        "    \n",
        "    return br_concat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U2BEZ6W8zAuA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#INCEPTION MODULE C\n",
        "def inceptionC(name,base_inp):\n",
        "  with tf.variable_scope(name):\n",
        "    br1 = tf.layers.conv2d(base_inp,filters=448,kernel_size=1,strides=1,padding='same',activation=tf.nn.relu,krenel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer()tf.constant(0.0005,dtype=tf.float32),name='b1_1x1')\n",
        "    br1 = tf.layers.conv2d(br1,filters=384,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,krenel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer()tf.constant(0.0005,dtype=tf.float32),name='b1_3x3')\n",
        "    br1_1 = tf.layers.conv2d(br1,filters=384,kernel_size=[1,3],strides=1,padding='same',activation=tf.nn.relu,krenel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer()tf.constant(0.0005,dtype=tf.float32),name='b1_1x3')\n",
        "    br1_2 = tf.layers.conv2d(br1,filters=384,kernel_size=[3,1],strides=1,padding='same',activation=tf.nn.relu,krenel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer()tf.constant(0.0005,dtype=tf.float32),name='b1_3x1')\n",
        "    \n",
        "    br2 = tf.layers.conv2d(base_inp,filters=384,kernel_size=1,strides=1,padding='same',activation=tf.nn.relu,krenel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer()tf.constant(0.0005,dtype=tf.float32),name='b2_1x1')\n",
        "    br2_1 = tf.layers.conv2d(br2,filters=384,kernel_size=[1,3],strides=1,padding='same',activation=tf.nn.relu,krenel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer()tf.constant(0.0005,dtype=tf.float32),name='b2_1x3')\n",
        "    br2_2 = tf.layers.conv2d(br2,filters=384,kernel_size=[3,1],strides=1,padding='same',activation=tf.nn.relu,krenel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer()tf.constant(0.0005,dtype=tf.float32),name='b2_3x1')\n",
        "    \n",
        "    br3 = tf.layers.average_pooling2d(base_inp,pool_size=3,strides=1,padding='same',name='incC_b3_avg_pool')\n",
        "    br3 = tf.layers.conv2d(br3,filters=192,kernel_size=1,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b3_1x1')\n",
        "    \n",
        "    br4 = tf.layers.conv2d(base_inp,filters=320,kernel_size=1,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='b4_1x1')\n",
        "    \n",
        "    br_concat = tf.concat([br1_1,br1_2,br2_1,br2_2,br3,br4],3,name=name+'concat')\n",
        "    \n",
        "    return br_concat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "65gp-n_ErCWd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#INCEPTION-V2\n",
        "with tf.device('/gpu:0'):\n",
        "  #STEM\n",
        "  #28X28\n",
        "  conv1 = tf.layers.conv2d(tf.reshape(x,shape=[-1,28,28,1]),filters=32,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='conv1')\n",
        "  conv2 = tf.layers.conv2d(conv1,filters=32,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='conv2')\n",
        "  conv3 = tf.layers.conv2d(conv2,filters=64,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='conv3')\n",
        "  pool1 = tf.layers.max_pooling2d(conv3,pool_size=3,strides=1,padding='same',name='pool1')\n",
        "  #28X28X64\n",
        "  conv4 = tf.layers.conv2d(pool1,filters=80,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='conv4')\n",
        "  conv5 = tf.layers.conv2d(conv4,filters=192,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='conv5')\n",
        "  conv6 = tf.layers.conv2d(conv5,filters=256,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='conv6')\n",
        "  #28X28X256\n",
        "  #Inception Module A\n",
        "  incA1 = inceptionA('incA1',conv6)\n",
        "  incA2 = inceptionA('incA2',incA1)\n",
        "  incA3 = inceptionA('incA3',incA2)\n",
        "  #28X28X288\n",
        "  #GRID SIZE REDUCTION\n",
        "  post_incA_b1 = tf.layers.conv2d(incA3,filters=384,kernel_size=3,strides=2,padding='valid',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='post_incA_b1')\n",
        "  \n",
        "  post_incA_b2 = tf.layers.conv2d(incA3,filters=64,kernel_size=1,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='post_incA_b2_1x1')\n",
        "  post_incA_b2 = tf.layers.conv2d(post_incA_b2,filters=96,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='post_incA_b2_3x3_1')\n",
        "  post_incA_b2 = tf.layers.conv2d(post_incA_b2,filters=96,kernel_size=3,strides=2,padding='valid',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='post_incA_b2_3x3_2')\n",
        "  \n",
        "  post_incA_b3 = tf.layers.max_pooling2d(incA3,pool_size=3,strides=2,padding='valid',name='post_incA_maxpool')\n",
        "  \n",
        "  incB_base = tf.concat([post_incA_b1,post_incA_b2,post_incA_b3],axis=3,name='incB_base')\n",
        "  #14X14X768\n",
        "  #Inception Module B\n",
        "  incB1 = inceptionB('incB1',incB_base)\n",
        "  incB2 = inceptionB('incB2',incB1)\n",
        "  incB3 = inceptionB('incB3',incB2)\n",
        "  #incB4 = inceptionB('incB4',incB3)\n",
        "  #incB5 = inceptionB('incB5',incB4)\n",
        "  #14X14X768\n",
        "  \n",
        "  #AUXILIARY SOFTMAX\n",
        "  #10X1\n",
        "  aux_head = tf.layers.average_pooling2d(incB3,pool_size=3,strides=2,padding='valid',name='aux_head_avg_pool')\n",
        "  #6X6X768\n",
        "  aux_head = tf.layers.conv2d(aux_head,filters=128,kernel_size=1,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='aux_head_1x1')\n",
        "  #6X6X128\n",
        "  aux_head = tf.layers.conv2d(aux_head,filters=384,kernel_size=3,strides=2,padding='valid',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='aux_head_3x3')\n",
        "  #2X2X384\n",
        "  aux_head = tf.layers.dense(aux_head,units=768,activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initiaizer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='aux_head_fc')\n",
        "  #768X1\n",
        "  aux_logits = tf.layers.dense(aux_head,units=10,activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initiaizer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='aux_head_logits')\n",
        "  #10X1\n",
        "  \n",
        "  #GRID SIZE REDUCTION\n",
        "  post_incB_b1 = tf.layers.conv2d(incB3,filters=192,kernel_size=1,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='post_incB_b1_1x1')\n",
        "  post_incB_b1 = tf.layers.conv2d(post_incB_b1,filters=320,kernel_size=3,strides=2,padding='valid',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='post_incB_b1_3x3')\n",
        "  \n",
        "  post_incB_b2 = tf.layers.conv2d(incB3,filters=192,kernel_size=1,strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='post_incB_b2_1x1')\n",
        "  post_incB_b2 = tf.layers.conv2d(post_incB_b2,filters=192,kernel_size=[1,3],strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='post_incB_b2_1x3')\n",
        "  post_incB_b2 = tf.layers.conv2d(post_incB_b2,filters=192,kernel_size=[3,1],strides=1,padding='same',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='post_incB_b2_3x1')\n",
        "  post_incB_b2 = tf.layers.conv2d(post_incB_b2,filters=192,kernel_size=3,strides=2,padding='valid',activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),kernel_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.0005,dtype=tf.float32)),name='post_incB_b2_3x3')\n",
        "  \n",
        "  post_incB_b3 = tf.layers.max_pooling2d(incB3,pool_size=3,strides=2,padding='valid',name='post_incB_max_pool')\n",
        "  \n",
        "  incC_base = tf.concat([post_incB_b1,post_incB_b2,post_incB_b3],3,name='incC_base')\n",
        "  #7X7X1280\n",
        "  #Inception Module C\n",
        "  incC1 = inceptionC('incC1',incC_base)\n",
        "  incC2 = inceptionC('incC2',incC1)\n",
        "  #incC3 = inceptionC('incC3',incC2)\n",
        "  #7X7X2048\n",
        "  pool2 = tf.layers.average_pooling2d(incC2,pool_size=7,strides=1,padding='valid',name='pool2_avg_pool')\n",
        "  #1X1X2048\n",
        "  pool2_flatten = tf.layers.flatten(pool2)\n",
        "  dropout1 = tf.layers.dropout(pool2,rate=keep_prob,training=mode) #introduce new placeholder\n",
        "  logits = tf.layers.dense(dropout1,units=10)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U49DScNLBEWc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yXXsysyGBNdX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 25\n",
        "ne = 0\n",
        "tracc=[]\n",
        "valacc=[]\n",
        "trerr=[]\n",
        "valerr=[]\n",
        "batch_size = 100\n",
        "numiter = 2000\n",
        "index=np.arange(200000)\n",
        "index_t = np.arange(40000)\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  while(ne<epochs):\n",
        "    print 'Epoch:: ',ne+1,'-->'\n",
        "    stime = time.time()\n",
        "    if ne != 0:\n",
        "\t\t  np.random.shuffle(index)\n",
        "\t\t  images = images[index]\n",
        "\t\t  labels = labels[index]\n",
        "    for niter in xrange(numiter):\n",
        "      #if (niter+1)%100==0:\n",
        "        #print 'Iter::',niter+1,'---------'\n",
        "      offset = niter*batch_size\n",
        "      x_iter, y_iter = np.array(images[offset:offset+batch_size,:,:]), np.array(labels[offset:offset+batch_size])\n",
        "      \n",
        "      #Training Dict\n",
        "      feed_trdict={x:x_iter,y:y_iter,keep_prob:0.8,mode:True}\n",
        "\t\t  \n",
        "      #Train\n",
        "      sess.run(opt_op,feed_dict=feed_trdict)\n",
        "    \n",
        "    \n",
        "    #Calculate accuracy of Training set\n",
        "    print 'Calculating Loss and Accuracy...'\n",
        "    tr_loss = sess.run(cost,feed_dict=feed_trdict)\n",
        "    tr_acc = sess.run(top1_acc,feed_dict = feed_trdict)\n",
        "    \n",
        "    #Calculate accuracy of Validation set\n",
        "    np.random.shuffle(index_t)\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "    for j in xrange(400):\n",
        "      val_loss += sess.run(cost,feed_dict = {x:test_imgs[index_t[j*100:(j+1)*100]],y:test_labels[index_t[j*100:(j+1)*100]],keep_prob:1.0,mode:False})\n",
        "      val_acc += sess.run(top1_acc,feed_dict = {x:test_imgs[index_t[j*100:(j+1)*100]],y:test_labels[index_t[j*100:(j+1)*100]],keep_prob:1.0,mode:False})\n",
        "    val_loss/=400\n",
        "    val_acc/=400\n",
        "    print 'Appending values'\n",
        "    valacc.append(val_acc)\n",
        "    tracc.append(tr_acc)\n",
        "    trerr.append(tr_loss)\n",
        "    valerr.append(val_loss)\n",
        "    print 'Epoch..',ne+1,'...'\n",
        "    print 'Training cost::',tr_loss,' Validation Cost:: ',val_loss\n",
        "    print 'Training accuracy::',tr_acc*100,'%',' Validation accuracy::',val_acc*100,'%'\n",
        "    print 'Time reqd.::',(time.time()-stime)/60,'mins...'\n",
        "    ne = ne + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LRBbR_Ledpvp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(tracc)\n",
        "plt.plot(valacc)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p_EYAb7pekES",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(trerr)\n",
        "plt.plot(valerr)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hbN0micRen_M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#INFERENCE\n",
        "test_acc=[]\n",
        "tindex = np.arange(40000)\n",
        "\n",
        "for nepoch in xrange(100):\n",
        "  stime = time.time()\n",
        "\tif nepoch !=0:\n",
        "\t\tnp.random.shuffle(tindex)\n",
        "\t\ttest_x = test_x[index]\n",
        "\t\ttest_y = test_y[index]\n",
        "\ttest_loss = sess.run(cost,feed_dict = {feed_images:test_x,feed_labels:test_y,keep_prob:1.0,mode=False})\n",
        "\ttest_acc = sess.run(acc,feed_dict = {feed_images:test_x,feed_labels:test_y,keep_prob:1.0,mode=False})\n",
        "\tprint 'Epoch..',ne+1,'...'\n",
        "\tprint 'Time reqd.::',(time.time()-stime)/60,'mins...'\n",
        "  test_acc.append(test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MUM99b-DyW35",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AGuCeZZQ5iuX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#SAVE MODEL\n",
        "graph = tf.get_default_graph()\n",
        "\n",
        "with graph.as_default():\n",
        "  input_dict = {\"x\":x,\"y\":y,\"keep_prob\":keep_prob,\"aug_img\":aug_img,\"mode\":mode}\n",
        "  output_dict = {\"logits\":logits,\"aux_logits\":aux_logits}\n",
        "  tf.saved_model.simple_save(sess,'/mnist_saved_model/',input_dict,output_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "syEL4eNuyjs7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}